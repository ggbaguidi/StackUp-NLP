{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e39bff",
   "metadata": {},
   "source": [
    "# Introduction to NLP in Python\n",
    "## Quest 2: Vectorization\n",
    "\n",
    "### Bag-of-Words Model (BOW)\n",
    "\n",
    "\n",
    "In Python, we can use the scikit-learn library to create BoW vectors from text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb2711",
   "metadata": {},
   "source": [
    "1. If it has not been installed, run the following line to install scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977ec549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (1.2.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from scikit-learn) (1.1.1)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.1\n",
      "    Uninstalling scikit-learn-1.2.1:\n",
      "      Successfully uninstalled scikit-learn-1.2.1\n",
      "Successfully installed scikit-learn-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0377801",
   "metadata": {},
   "source": [
    "2. Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0769fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5289f",
   "metadata": {},
   "source": [
    "3. Create an instance of the CountVectorizer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c684896",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3ca6f",
   "metadata": {},
   "source": [
    "4. Load your text data into a list or an array. Here, we have provided some sentences for you to work with. Feel free to add on to the sentences or come up with your own sample text to experiment more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57875cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = [\n",
    "    \"Supervised learning is a type of machine learning\",\n",
    "    \"Unsupervised learning is another type of machine learning\",\n",
    "    \"Machine learning algorithms can be used for NLP\",\n",
    "    \"NLP is a subfield of machine learning\",\n",
    "    \"Sentiment analysis is a type of NLP application\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6256d9b4",
   "metadata": {},
   "source": [
    "5. Next, we fit and transform the text data. Fitting the vectorizer on the text data creates the vocabulary, while the transform method transforms the text data into a BoW representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd4988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = count_vectorizer.fit_transform(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f926c",
   "metadata": {},
   "source": [
    "The resulting `bow` object is a sparse matrix that represents the text data in BoW form. Go ahead and print the contents of this matrix to see how the words in the sample text have been mapped to their respective indices in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac4bf6e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 2 1 0 1 0 0 1 1 0 0]\n",
      " [0 0 1 0 0 0 0 1 2 1 0 1 0 0 0 1 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0]\n",
      " [0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91649d6a",
   "metadata": {},
   "source": [
    "Each row in the matrix corresponds to a sentence in the vocabulary. The values in the matrix represent the frequency of each word in each sentence. \n",
    "\n",
    "That's it! You now know how to use the Bag-of-Words vectorizer in Python. **Head back to the StackUp platform, where we continue with another vectorization technique.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb47ea",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The scikit-learn library provides a convenient implementation of the TF-IDF algorithm. \n",
    "1. First, run the cell below to import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46736703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2365b",
   "metadata": {},
   "source": [
    "2. Define the text that you want to analyze. This can be a list or array of strings, or even a Pandas DataFrame containing text data for a larger dataset. \n",
    "\n",
    "Here, lets work with a slightly larger set of data than before. Feel free to include more sentences or import your own Pandas DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3619e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"Supervised learning is a type of machine learning.\",\n",
    "    \"Unsupervised learning is another type of machine learning.\",\n",
    "    \"Machine learning algorithms can be used for a wide range of applications, such as image recognition and natural language processing.\",\n",
    "    \"One of the most popular machine learning algorithms is the decision tree.\",\n",
    "    \"Random forest is an ensemble learning method that combines multiple decision trees.\",\n",
    "    \"Support vector machines are a powerful class of machine learning algorithms used for classification and regression tasks.\",\n",
    "    \"Neural networks are a type of machine learning algorithm inspired by the structure and function of the human brain.\",\n",
    "    \"Deep learning is a subfield of machine learning that involves neural networks with many layers.\",\n",
    "    \"Transfer learning is a technique in machine learning where a model trained on one task is used to improve performance on another related task.\",\n",
    "    \"Natural Language Processing (NLP) is a subfield of machine learning that deals with the interaction between computers and humans in natural language.\",\n",
    "    \"Text classification is an important task in NLP that involves assigning a document to one or more predefined categories.\",\n",
    "    \"Named Entity Recognition (NER) is another important task in NLP that involves identifying and extracting entities such as people, organizations, and locations from text data.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5cef4",
   "metadata": {},
   "source": [
    "3. Create an instance of the TfidfVectorizer class. You can specify any parameters you want to use. Some common parameters include:\n",
    "- `max_features`: the maximum number of features (unique words) to include in the TF-IDF matrix\n",
    "- `stop_words`: a list of words to exclude from the TF-IDF calculation, such as common stop words like \"the\" and \"and\"\n",
    "\n",
    "If you are keen to know more, check out other available parameters [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Feel free to experiment with the parameters and changing the values in the vectorizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a6d3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f65fa",
   "metadata": {},
   "source": [
    "4. Fit and transform the text into a TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a89793",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615f05e",
   "metadata": {},
   "source": [
    "5. Print the resulting matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd233e02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.52146104 0.         0.28202368 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.64158678\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48673138 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.52146104 0.         0.28202368 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48673138 0.64158678 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.26082995 0.34381397 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34381397 0.         0.         0.         0.         0.\n",
      "  0.29527143 0.         0.13972045 0.         0.15113105 0.\n",
      "  0.         0.         0.         0.         0.29527143 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29527143 0.         0.34381397\n",
      "  0.29527143 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26082995\n",
      "  0.         0.34381397]\n",
      " [0.         0.39592654 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.44820694 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.21208851 0.         0.22940921 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.52189204 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.52189204 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35572313 0.         0.         0.\n",
      "  0.30549916 0.         0.         0.35572313 0.         0.\n",
      "  0.         0.35572313 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14456014 0.         0.         0.\n",
      "  0.35572313 0.         0.35572313 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.35572313 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35572313 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.24947888 0.         0.         0.         0.\n",
      "  0.32885151 0.28242149 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.13363995 0.         0.14455396 0.32885151\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.32885151 0.         0.         0.         0.\n",
      "  0.         0.32885151 0.         0.         0.         0.\n",
      "  0.32885151 0.         0.32885151 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.24947888\n",
      "  0.32885151 0.        ]\n",
      " [0.34484776 0.         0.         0.         0.34484776 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.34484776 0.34484776 0.         0.\n",
      "  0.         0.         0.         0.34484776 0.         0.\n",
      "  0.         0.         0.14014057 0.         0.15158547 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29615926 0.29615926 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.34484776 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.26161422 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.42100068 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31938664\n",
      "  0.         0.42100068 0.34217577 0.         0.18506018 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.36156028 0.36156028 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.36156028 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30526042 0.         0.         0.\n",
      "  0.         0.         0.24810583 0.         0.13418398 0.\n",
      "  0.         0.30526042 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.30526042\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30526042 0.         0.         0.\n",
      "  0.         0.46316362 0.         0.30526042 0.         0.30526042\n",
      "  0.30526042 0.         0.         0.         0.         0.23158181\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28502321 0.         0.28502321\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.28502321 0.\n",
      "  0.         0.         0.         0.         0.28502321 0.\n",
      "  0.48956249 0.         0.11582884 0.         0.12528827 0.\n",
      "  0.         0.         0.         0.         0.48956249 0.\n",
      "  0.         0.         0.21622911 0.         0.         0.\n",
      "  0.         0.         0.         0.24478124 0.         0.\n",
      "  0.         0.         0.         0.         0.24478124 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.35490322 0.         0.35490322\n",
      "  0.         0.30479501 0.         0.         0.         0.\n",
      "  0.         0.         0.35490322 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30479501 0.         0.         0.         0.26924266\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26924266 0.         0.         0.\n",
      "  0.         0.         0.35490322 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.26924266 0.         0.         0.30479501 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26784288 0.\n",
      "  0.         0.         0.         0.         0.26784288 0.26784288\n",
      "  0.26784288 0.         0.         0.         0.         0.26784288\n",
      "  0.         0.23002658 0.         0.         0.         0.20319549\n",
      "  0.         0.         0.         0.26784288 0.         0.\n",
      "  0.         0.         0.         0.26784288 0.         0.26784288\n",
      "  0.         0.         0.20319549 0.26784288 0.26784288 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.23002658 0.         0.         0.         0.         0.\n",
      "  0.         0.20319549 0.         0.         0.23002658 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59902e55",
   "metadata": {},
   "source": [
    "The resulting matrix is printed using the `toarray()` method, which converts the sparse matrix to a dense matrix for easier viewing.\n",
    "\n",
    "That's it for the TF-IDF technique! **Switch back over to the StackUp platform, where we cover the third and final vectorization method.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e18e2a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Word Embeddings\n",
    "\n",
    "Here, we will use the Gensim Python library to create word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd265ca",
   "metadata": {},
   "source": [
    "1. If it has not been installed, run the following line to install Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f33b6084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (4.3.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from gensim) (1.10.0)\n",
      "Collecting pyfume\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m238.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m414.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Collecting fst-pso\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting simpful\n",
      "  Downloading simpful-2.10.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting miniful\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.0.4)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=42db753bddc163a296067b0f4e3b7f070aa05cecab3083025c58561ab64f3b2b\n",
      "  Stored in directory: /home/ggbaguidi/.cache/pip/wheels/01/02/ee/df0699282986903a384b69aab4413af9efd26b3612b5dccc9e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3513 sha256=db3652ae84e76da8775e3c41a1e2107cc09e12accc459634486c063d26ac9b80\n",
      "  Stored in directory: /home/ggbaguidi/.cache/pip/wheels/43/aa/48/5c66b931ff013ad19774081aa19656637af5c0cc33b5494b30\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 miniful-0.0.6 pyfume-0.2.25 simpful-2.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3091de",
   "metadata": {},
   "source": [
    "2. Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0daf62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2dee7",
   "metadata": {},
   "source": [
    "3. Train a Word2Vec model using a tokenized text corpus. Word2Vec takes in tokens as its input, so the `word_tokenize` method from the previous quest is used here. We will make use of the `text_data` list from the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94cefa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"Supervised learning is a type of machine learning.\",\n",
    "    \"Unsupervised learning is another type of machine learning.\",\n",
    "    \"Machine learning algorithms can be used for a wide range of applications, such as image recognition and natural language processing.\",\n",
    "    \"One of the most popular machine learning algorithms is the decision tree.\",\n",
    "    \"Random forest is an ensemble learning method that combines multiple decision trees.\",\n",
    "    \"Support vector machines are a powerful class of machine learning algorithms used for classification and regression tasks.\",\n",
    "    \"Neural networks are a type of machine learning algorithm inspired by the structure and function of the human brain.\",\n",
    "    \"Deep learning is a subfield of machine learning that involves neural networks with many layers.\",\n",
    "    \"Transfer learning is a technique in machine learning where a model trained on one task is used to improve performance on another related task.\",\n",
    "    \"Natural Language Processing (NLP) is a subfield of machine learning that deals with the interaction between computers and humans in natural language.\",\n",
    "    \"Text classification is an important task in NLP that involves assigning a document to one or more predefined categories.\",\n",
    "    \"Named Entity Recognition (NER) is another important task in NLP that involves identifying and extracting entities such as people, organizations, and locations from text data.\"\n",
    "]\n",
    "\n",
    "tokens_in_text = [nltk.word_tokenize(sentence.lower()) for sentence in text_data] \n",
    "\n",
    "model = Word2Vec(tokens_in_text, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b81db0",
   "metadata": {},
   "source": [
    "The cell above returns the list of sentences into individual tokens after converting the words to lowercase, and is passed into the Word2Vec model. \n",
    "\n",
    "The `min_count` parameter ignores all words with a total frequency lower than this. In this case, all words that only appear once is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d7921",
   "metadata": {},
   "source": [
    "4. Once your model is trained, you can access the word embeddings for any word like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2066d4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.2433320e-03  4.1890572e-04 -9.6239574e-04 -3.2622248e-03\n",
      "  4.9813832e-03 -9.1374218e-03  5.1592113e-03  5.7440903e-03\n",
      " -2.4666542e-03 -4.2066104e-03 -7.3827682e-03 -6.4108432e-03\n",
      " -5.1381472e-03  3.1858739e-03 -4.7803563e-03 -1.2228384e-03\n",
      " -8.9032417e-03 -3.5037154e-03 -8.0953800e-04 -8.0476720e-03\n",
      "  3.2537808e-03  9.7621465e-03  4.4832961e-03  1.8484986e-03\n",
      " -4.8418064e-03 -1.1118491e-03  2.2252002e-03  1.7033379e-03\n",
      "  8.7847020e-03 -9.2013582e-04  7.0528258e-03  5.7957778e-03\n",
      "  3.1277214e-03  7.2565642e-03 -4.9505257e-03 -3.9364714e-03\n",
      "  6.9563957e-03  8.5821832e-03  3.1465974e-03 -4.1753957e-03\n",
      "  7.6282239e-03  2.6987009e-03 -2.6221786e-04  8.8245040e-03\n",
      "  4.3581417e-03  3.9333361e-03 -3.9797589e-05 -8.9332564e-03\n",
      " -3.0806521e-03  2.4464622e-03 -5.6178658e-03  7.0744818e-03\n",
      "  7.4318070e-03 -8.9356452e-03 -5.3773653e-03 -6.0308459e-03\n",
      " -6.4305880e-04 -1.8272620e-03 -2.0672381e-03 -2.2206558e-03\n",
      "  3.5930267e-03  9.5866909e-03 -4.5388569e-03 -8.2720798e-03\n",
      "  3.7101971e-03 -6.3875900e-03  6.6857054e-03  2.6999521e-03\n",
      " -7.9799686e-03 -7.7022929e-03 -9.2647877e-03  2.4025515e-03\n",
      "  2.7865313e-03  2.6320789e-03  1.9728293e-04  1.0295683e-03\n",
      "  3.0974834e-03 -1.5389993e-03 -8.0493242e-03 -6.2121963e-04\n",
      "  3.7043523e-03 -2.6478982e-03 -2.1919645e-03 -2.2312822e-03\n",
      "  7.2559710e-03 -3.0972599e-03  9.5456075e-03 -3.8740141e-03\n",
      " -9.9790515e-03 -6.5089334e-03 -4.3607019e-03  1.2760976e-03\n",
      "  5.1270239e-03  2.1124743e-03  9.1222869e-03  9.1652921e-04\n",
      "  5.6929351e-03  1.3505618e-04 -2.0971072e-03  6.1982679e-03]\n"
     ]
    }
   ],
   "source": [
    "vector = model.wv['class']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5f80a",
   "metadata": {},
   "source": [
    "This will return a vector representing the word 'class' in the embedding space. The attribute `wv` of a Word2Vec model stands for \"word vector\".\n",
    "\n",
    "And that sums up the 3 techniques for vectorization in NLP! **Return back to the StackUp platform,** where we wrap up the quest and prepare the deliverables for submission. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
