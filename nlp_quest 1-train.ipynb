{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06f64d6",
   "metadata": {},
   "source": [
    "# Introduction to NLP in Python\n",
    "## Quest 1: NLP Basics for Text Preprocessing\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenizers divide strings into lists of substrings. After installing the nltk library, let's import the library along with these two built-in methods, *sent_tokenize* and *word_tokenize*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f673677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f79c4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ggbaguidi/Documents/IATools/Anaconda3/nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c6b267",
   "metadata": {},
   "source": [
    "1. `sent_tokenize`\n",
    "\n",
    "The first method, `sent_tokenize`, splits the given text into sentences. This is useful especially if you are dealing with bigger chunks of text with longer sentences.\n",
    "\n",
    "We will make use of the following sample paragraph about NLP in the healthcare industry. Run the cell below to check out the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6c0ebb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the healthcare industry, NLP is used to analyze large amounts of healthcare-related data.',\n",
       " 'This includes clinical notes and medical imaging reports, and many more.',\n",
       " 'With the help of NLP, healthcare providers can quickly and accurately identify patterns and insights from patient data.',\n",
       " 'For example, NLP can predict patient outcomes, such as the likelihood of readmission or the risk of developing a particular condition.',\n",
       " 'NLP can also be used to extract key information from medical imaging reports.',\n",
       " 'An example can be the size and location of tumours.',\n",
       " 'This can help healthcare providers make more informed treatment decisions.',\n",
       " 'Overall, NLP is a powerful tool that can help improve patient outcomes and enhance the quality of care in the healthcare industry.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'In the healthcare industry, NLP is used to analyze large amounts of healthcare-related data. This includes clinical notes and medical imaging reports, and many more. With the help of NLP, healthcare providers can quickly and accurately identify patterns and insights from patient data. For example, NLP can predict patient outcomes, such as the likelihood of readmission or the risk of developing a particular condition. NLP can also be used to extract key information from medical imaging reports. An example can be the size and location of tumours. This can help healthcare providers make more informed treatment decisions. Overall, NLP is a powerful tool that can help improve patient outcomes and enhance the quality of care in the healthcare industry.'\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650cb69",
   "metadata": {},
   "source": [
    "If you encounter the \"Resource punkt not found\" error when running the above cell, you can run the following command `nltk.download('punkt')`\n",
    "<br/><br/>\n",
    "\n",
    "2. `word_tokenize`\n",
    "\n",
    "Likewise, the `word_tokenize` method tokenizes each individual word in the paragraph. Run the cell below to compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b00bd69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'the',\n",
       " 'healthcare',\n",
       " 'industry',\n",
       " ',',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'used',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'healthcare-related',\n",
       " 'data',\n",
       " '.',\n",
       " 'This',\n",
       " 'includes',\n",
       " 'clinical',\n",
       " 'notes',\n",
       " 'and',\n",
       " 'medical',\n",
       " 'imaging',\n",
       " 'reports',\n",
       " ',',\n",
       " 'and',\n",
       " 'many',\n",
       " 'more',\n",
       " '.',\n",
       " 'With',\n",
       " 'the',\n",
       " 'help',\n",
       " 'of',\n",
       " 'NLP',\n",
       " ',',\n",
       " 'healthcare',\n",
       " 'providers',\n",
       " 'can',\n",
       " 'quickly',\n",
       " 'and',\n",
       " 'accurately',\n",
       " 'identify',\n",
       " 'patterns',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'patient',\n",
       " 'data',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'NLP',\n",
       " 'can',\n",
       " 'predict',\n",
       " 'patient',\n",
       " 'outcomes',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'likelihood',\n",
       " 'of',\n",
       " 'readmission',\n",
       " 'or',\n",
       " 'the',\n",
       " 'risk',\n",
       " 'of',\n",
       " 'developing',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'condition',\n",
       " '.',\n",
       " 'NLP',\n",
       " 'can',\n",
       " 'also',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'extract',\n",
       " 'key',\n",
       " 'information',\n",
       " 'from',\n",
       " 'medical',\n",
       " 'imaging',\n",
       " 'reports',\n",
       " '.',\n",
       " 'An',\n",
       " 'example',\n",
       " 'can',\n",
       " 'be',\n",
       " 'the',\n",
       " 'size',\n",
       " 'and',\n",
       " 'location',\n",
       " 'of',\n",
       " 'tumours',\n",
       " '.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'help',\n",
       " 'healthcare',\n",
       " 'providers',\n",
       " 'make',\n",
       " 'more',\n",
       " 'informed',\n",
       " 'treatment',\n",
       " 'decisions',\n",
       " '.',\n",
       " 'Overall',\n",
       " ',',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'tool',\n",
       " 'that',\n",
       " 'can',\n",
       " 'help',\n",
       " 'improve',\n",
       " 'patient',\n",
       " 'outcomes',\n",
       " 'and',\n",
       " 'enhance',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'care',\n",
       " 'in',\n",
       " 'the',\n",
       " 'healthcare',\n",
       " 'industry',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908050b",
   "metadata": {},
   "source": [
    "Additionally, feel free to experiment with different sentences and pieces of text and passing them through each tokenizer. \n",
    "\n",
    "There are many more types of tokenizers in the nltk library itself, catered to producing various tokens based on the type of data that is needed. You can learn more about tokenizers from the nltk documentation [here](https://www.nltk.org/api/nltk.tokenize.html).\n",
    "\n",
    "Return back to the StackUp platform, where we will continue on with the quest.\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a894c1a",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "\n",
    "Stop words are the common words which don't really add much meaning to the text. Some stop words in English includes conjunctions such as for, and, but, or, yet, so, and articles such as a, an, the.\n",
    "\n",
    "NLTK has pre-defined stop words for English. Let's go ahead and import it by running in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577c7d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ggbaguidi/Documents/IATools/Anaconda3/nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226b1dd",
   "metadata": {},
   "source": [
    "The list stopwords now contains the NLTK predefined stop words. Using the tokenized text from earlier, let's remove the stop words and return the remaining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e9e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'healthcare', 'industry', ',', 'NLP', 'used', 'analyze', 'large', 'amounts', 'healthcare-related', 'data', '.', 'This', 'includes', 'clinical', 'notes', 'medical', 'imaging', 'reports', ',', 'many', '.', 'With', 'help', 'NLP', ',', 'healthcare', 'providers', 'quickly', 'accurately', 'identify', 'patterns', 'insights', 'patient', 'data', '.', 'For', 'example', ',', 'NLP', 'predict', 'patient', 'outcomes', ',', 'likelihood', 'readmission', 'risk', 'developing', 'particular', 'condition', '.', 'NLP', 'also', 'used', 'extract', 'key', 'information', 'medical', 'imaging', 'reports', '.', 'An', 'example', 'size', 'location', 'tumours', '.', 'This', 'help', 'healthcare', 'providers', 'make', 'informed', 'treatment', 'decisions', '.', 'Overall', ',', 'NLP', 'powerful', 'tool', 'help', 'improve', 'patient', 'outcomes', 'enhance', 'quality', 'care', 'healthcare', 'industry', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens_no_stopwords = [i for i in tokens if i not in stopwords]\n",
    "print(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484e6cc",
   "metadata": {},
   "source": [
    "Now, lets head back to the StackUp platform, where we cover the third preprocessing technique in this quest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3c19d",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "Here, we will experiment using the PorterStemmer and WordNetLemmatizer. Recall from the quest that stemming removes the suffix from the word while lemmatization takes into account the context and what the word means in the sentence.\n",
    "\n",
    "Play along with different words to compare the outputs produced by a stemmer and a lemmatizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ccbc57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ggbaguidi/Documents/IATools/Anaconda3/nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/ggbaguidi/Documents/IATools/Anaconda3/nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# run these lines if they have yet to be downloaded.\n",
    "# once downloaded, you can comment out the lines.\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa877b4",
   "metadata": {},
   "source": [
    "Let's test both methods on various pluralised words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b16dff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming results:  ['appl', 'octopus', 'categori', 'criteria', 'tomato', 'matric', 'hypothes', 'radii', 'alga', 'cacti']\n",
      "Lemmatization results;  ['apple', 'octopus', 'category', 'criterion', 'tomato', 'matrix', 'hypothesis', 'radius', 'algae', 'cactus']\n"
     ]
    }
   ],
   "source": [
    "plurals = ['apples', 'octopuses', 'categories', 'criteria', 'tomatoes', 'matrices', 'hypotheses', 'radii', 'algae', 'cacti']\n",
    "\n",
    "plurals_stem = [stemmer.stem(plural) for plural in plurals]\n",
    "plurals_lemma = [lemma.lemmatize(plural) for plural in plurals]\n",
    "\n",
    "print(\"Stemming results: \", plurals_stem)\n",
    "print(\"Lemmatization results; \", plurals_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f0a6a",
   "metadata": {},
   "source": [
    "Compare the results produced above! The lemmatizer is more accurate when it comes to getting the root word of more complex plurals, however it is important to note that in the case of a large dataset, stemming comes in handy where performance is an issue. \n",
    "\n",
    "And that sums up the 3 techniques for text preprocessing in NLP! **Return back to the StackUp platform,** where we wrap up the quest and prepare the deliverables for submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ac2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
