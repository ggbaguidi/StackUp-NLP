{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4af5236",
   "metadata": {},
   "source": [
    "# Introduction to NLP in Python\n",
    "## Quest 3: Creating your own fake news classifier\n",
    "\n",
    "The rise of social media and the proliferation of digital news sources have made it easier than ever to spread misinformation and fake news. As a result, this prompts the need to be able to distinguish between real and fake news stories. Machine learning techniques offer a promising solution to this, by allowing us to classify news articles based on their content and other features.\n",
    "\n",
    "In this quest, we explore the use of machine learning classification methods to classify news articles as either real or fake. We will analyse the text of the news articles based on the natural language processing (NLP) methods learnt in previous quests, and evaluate the performance of our classifier using a variety of metrics. \n",
    "\n",
    "**Do take note:** Lines that contain underscores are for you to fill in with your code! Do remove the underscores before running the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe4d3d",
   "metadata": {},
   "source": [
    "##### Part 1: Data Preprocessing using NLP Techniques\n",
    "\n",
    "In this first section, we will first perform some exploratory data analysis on the data provided, and preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2052f9c",
   "metadata": {},
   "source": [
    "1. Import the necesary libraries and methods. The libraries listed below are the basic data science libraries that you can use, but feel free to import other libraries that you feel will help you along this quest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf12dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab298ea5",
   "metadata": {},
   "source": [
    "2. Read the dataset from the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d8c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fake_news_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caffac5",
   "metadata": {},
   "source": [
    "3. After loading the dataset, perform some primary exploratory data analysis to understand the dataset provided. You can use simple pandas methods and attributes such as `head()`, `shape` and `info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffa7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Haed of dataframe: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploratory data analysis to familiarize yourself with the data\n",
    "print(\"\\nHaed of dataframe: \\n\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97dc3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape: \n",
      " (20800, 5)\n",
      "\n",
      "Informations: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      20800 non-null  int64 \n",
      " 1   title   20242 non-null  object\n",
      " 2   author  18843 non-null  object\n",
      " 3   text    20761 non-null  object\n",
      " 4   label   20800 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nThe shape: \\n\", df.shape)\n",
    "print(\"\\nInformations: \\n\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0979e",
   "metadata": {},
   "source": [
    "4. Before proceeding, it is always a good measure to check if null values are present in the dataset or not. \n",
    "\n",
    "If there are null values in the DataFrame, use the `fillna` method to fill the null values with an empty string (i.e. \"\") Remember to specify the `axis=1` parameter to fill the missing values along the columns of the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6972347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fillna in module pandas.core.frame:\n",
      "\n",
      "fillna(value: 'Hashable | Mapping | Series | DataFrame' = None, *, method: 'FillnaOptions | None' = None, axis: 'Axis | None' = None, inplace: 'bool' = False, limit: 'int | None' = None, downcast: 'dict | None' = None) -> 'DataFrame | None' method of pandas.core.frame.DataFrame instance\n",
      "    Fill NA/NaN values using the specified method.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    value : scalar, dict, Series, or DataFrame\n",
      "        Value to use to fill holes (e.g. 0), alternately a\n",
      "        dict/Series/DataFrame of values specifying which value to use for\n",
      "        each index (for a Series) or column (for a DataFrame).  Values not\n",
      "        in the dict/Series/DataFrame will not be filled. This value cannot\n",
      "        be a list.\n",
      "    method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n",
      "        Method to use for filling holes in reindexed Series\n",
      "        pad / ffill: propagate last valid observation forward to next valid\n",
      "        backfill / bfill: use next valid observation to fill gap.\n",
      "    axis : {0 or 'index', 1 or 'columns'}\n",
      "        Axis along which to fill missing values. For `Series`\n",
      "        this parameter is unused and defaults to 0.\n",
      "    inplace : bool, default False\n",
      "        If True, fill in-place. Note: this will modify any\n",
      "        other views on this object (e.g., a no-copy slice for a column in a\n",
      "        DataFrame).\n",
      "    limit : int, default None\n",
      "        If method is specified, this is the maximum number of consecutive\n",
      "        NaN values to forward/backward fill. In other words, if there is\n",
      "        a gap with more than this number of consecutive NaNs, it will only\n",
      "        be partially filled. If method is not specified, this is the\n",
      "        maximum number of entries along the entire axis where NaNs will be\n",
      "        filled. Must be greater than 0 if not None.\n",
      "    downcast : dict, default is None\n",
      "        A dict of item->dtype of what to downcast if possible,\n",
      "        or the string 'infer' which will try to downcast to an appropriate\n",
      "        equal type (e.g. float64 to int64 if possible).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or None\n",
      "        Object with missing values filled or None if ``inplace=True``.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    interpolate : Fill NaN values using interpolation.\n",
      "    reindex : Conform object to new index.\n",
      "    asfreq : Convert TimeSeries to specified frequency.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n",
      "    ...                    [3, 4, np.nan, 1],\n",
      "    ...                    [np.nan, np.nan, np.nan, np.nan],\n",
      "    ...                    [np.nan, 3, np.nan, 4]],\n",
      "    ...                   columns=list(\"ABCD\"))\n",
      "    >>> df\n",
      "         A    B   C    D\n",
      "    0  NaN  2.0 NaN  0.0\n",
      "    1  3.0  4.0 NaN  1.0\n",
      "    2  NaN  NaN NaN  NaN\n",
      "    3  NaN  3.0 NaN  4.0\n",
      "    \n",
      "    Replace all NaN elements with 0s.\n",
      "    \n",
      "    >>> df.fillna(0)\n",
      "         A    B    C    D\n",
      "    0  0.0  2.0  0.0  0.0\n",
      "    1  3.0  4.0  0.0  1.0\n",
      "    2  0.0  0.0  0.0  0.0\n",
      "    3  0.0  3.0  0.0  4.0\n",
      "    \n",
      "    We can also propagate non-null values forward or backward.\n",
      "    \n",
      "    >>> df.fillna(method=\"ffill\")\n",
      "         A    B   C    D\n",
      "    0  NaN  2.0 NaN  0.0\n",
      "    1  3.0  4.0 NaN  1.0\n",
      "    2  3.0  4.0 NaN  1.0\n",
      "    3  3.0  3.0 NaN  4.0\n",
      "    \n",
      "    Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,\n",
      "    2, and 3 respectively.\n",
      "    \n",
      "    >>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
      "    >>> df.fillna(value=values)\n",
      "         A    B    C    D\n",
      "    0  0.0  2.0  2.0  0.0\n",
      "    1  3.0  4.0  2.0  1.0\n",
      "    2  0.0  1.0  2.0  3.0\n",
      "    3  0.0  3.0  2.0  4.0\n",
      "    \n",
      "    Only replace the first NaN element.\n",
      "    \n",
      "    >>> df.fillna(value=values, limit=1)\n",
      "         A    B    C    D\n",
      "    0  0.0  2.0  2.0  0.0\n",
      "    1  3.0  4.0  NaN  1.0\n",
      "    2  NaN  1.0  NaN  3.0\n",
      "    3  NaN  3.0  NaN  4.0\n",
      "    \n",
      "    When filling using a DataFrame, replacement happens along\n",
      "    the same column names and same indices\n",
      "    \n",
      "    >>> df2 = pd.DataFrame(np.zeros((4, 4)), columns=list(\"ABCE\"))\n",
      "    >>> df.fillna(df2)\n",
      "         A    B    C    D\n",
      "    0  0.0  2.0  0.0  0.0\n",
      "    1  3.0  4.0  0.0  1.0\n",
      "    2  0.0  0.0  0.0  NaN\n",
      "    3  0.0  3.0  0.0  4.0\n",
      "    \n",
      "    Note that column D is not affected since it is not present in df2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.fillna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa589935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           0\n",
      "title      558\n",
      "author    1957\n",
      "text        39\n",
      "label        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values and if any, fill them with an empty string \n",
    "print(df.isnull().sum())\n",
    "df.fillna(\"\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcf4b089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca23c16",
   "metadata": {},
   "source": [
    "5. For data preprocessing, we will focus on the 'text' column of the DataFrame, which contains the content of each news article. We will apply tokenization, the first text preprocessing method covered in Quest 1.\n",
    "\n",
    "Since the dataset has over 20,000 rows of data, it might take a while for the tokenization to finish running, depending on your machine specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e14b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenize the text given\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply the tokenize_text function to the 'text' column of the DataFrame and create a new column 'tokenized_text'\n",
    "df['tokenized_text'] = df['text'].map(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c8e3ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>[House, Dem, Aide, :, We, Didn, ’, t, Even, Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Ever, get, the, feeling, your, life, circles,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Why, the, Truth, Might, Get, You, Fired, Octo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Videos, 15, Civilians, Killed, In, Single, US...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Print, An, Iranian, woman, has, been, sentenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                              title              author  \\\n",
       "0  0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1  1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2  2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3  3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4  4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...     1   \n",
       "1  Ever get the feeling your life circles the rou...     0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...     1   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...     1   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...     1   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [House, Dem, Aide, :, We, Didn, ’, t, Even, Se...  \n",
       "1  [Ever, get, the, feeling, your, life, circles,...  \n",
       "2  [Why, the, Truth, Might, Get, You, Fired, Octo...  \n",
       "3  [Videos, 15, Civilians, Killed, In, Single, US...  \n",
       "4  [Print, An, Iranian, woman, has, been, sentenc...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def tokenize(text):\n",
    "#     return word_tokenize(text)\n",
    "\n",
    "# #tokenize(df[\"text\"].to_string())\n",
    "# print(df[\"text\"][:2].map(tokenize)[0])\n",
    "# df[\"text\"][0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b0fb96",
   "metadata": {},
   "source": [
    "6. With our new column containing the tokens of the text, we will dive into the second preprocessing step, which is to remove the stop words from the tokens.\n",
    "\n",
    "Create a list `stop_words` that contains the NLTK predefined stopwords. Recall that the stopwords module was imported in the first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec139f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13f3e2",
   "metadata": {},
   "source": [
    "7. Define a function that removes stop words from a list of tokens. Take note that the NLTK predefined stopwords are in lowercase, while some of the tokens in your current DataFrame contain uppercase alphabets. \n",
    "\n",
    "Apply the method you have defined to the `tokenized_text` column. You can choose to create a new column for these tokens without stopwords, or replace the `tokenized_text` column entirely to only contain tokens without stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6e06c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove stopwords from a list of tokens\n",
    "def remove_stopwords(tokens):\n",
    "    tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]     # replace and fill in underscores\n",
    "    return tokens_without_stopwords\n",
    "\n",
    "# Apply the remove_stopwords function to the 'tokenized_text' column\n",
    "df['tokenized_text'] = df['tokenized_text'].map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2d11448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [House, Dem, Aide, :, ’, Even, See, Comey, ’, ...\n",
       "1    [Ever, get, feeling, life, circles, roundabout...\n",
       "2    [Truth, Might, Get, Fired, October, 29, ,, 201...\n",
       "3    [Videos, 15, Civilians, Killed, Single, US, Ai...\n",
       "4    [Print, Iranian, woman, sentenced, six, years,...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0083e9",
   "metadata": {},
   "source": [
    "##### Part 2: Separating the dataset and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3fba8",
   "metadata": {},
   "source": [
    "8. Before we proceed, we will separate the dataset into features and targets. This allows us to clearly define the inputs and outputs of our model. \n",
    "\n",
    "The features are the independent variables that we use to predict the target variable, which is the dependent variable we want to predict. In this case, we are using the text from the article to determine if the article is reliable or unreliable. Reliable articles are labelled '0' in the `label` column while unreliable articles are labelled '1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8928d91f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate the data into features and targets\n",
    "X_df = df['tokenized_text']\n",
    "y_df = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270e470",
   "metadata": {},
   "source": [
    "9. Analyse the `y_df` data. Notice that the data type appears to be a `str` or `object` data type. \n",
    "\n",
    "Recall that we are looking to have a binary output which should only include numerical values. To train the model, we need to convert the label column into a numerical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70162a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = y_df.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360c894",
   "metadata": {},
   "source": [
    "10. As machine learning models take in numerical values for their inputs, we have to convert our feature data into numerical format as well. This is where we can incorporate our vectorization skills covered in Quest 2!\n",
    "\n",
    "Import the `TfidfVectorizer` and create a TfidfVectorizer object. Since the features we are working with are in tokens, we have to specify this in the parameter as the vectorizer takes in strings by default. \n",
    "\n",
    "We set the `tokenizer` parameter to a lambda function that simply returns each document as-is. We also set `lowercase=False` to ensure that the tokenization is not modified.\n",
    "\n",
    "After this, fit and transform the vectorizer on the tokenized documents `x_df`. This produces the TFIDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e065ab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggbaguidi/Documents/IATools/Anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Perform vectorization using the TFIDF Vectorizer and fit and transform the tokenized documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "tfidf = tfidf_vectorizer.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569c369",
   "metadata": {},
   "source": [
    "##### Part 3: Training and testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec5c74",
   "metadata": {},
   "source": [
    "11. Excellent! For the last part of this quest, we will be making use of a `LogisticRegression` model to create our fake news classifier. Logistic Regression has been covered in previous campaigns, but if you are new to this, don't worry as it is a relatively simple model to pick up. You can find out more about the different methods and attributes of Logistic Regression [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "We will split the data into a training set and a testing set to evaluate the performance of our Logistic Regression model. The training set is used to train the model, while the testing set is used to evaluate the model's performance on new data that is has not seen before. This helps us to determine how well the model will generalize to new data and avoid overfitting. \n",
    "\n",
    "This is important because the ultimate goal of a machine learning model is to make accurate predictions on new, real-world data. Without a testing set, we would have no way to evaluate the performance of our model on new data. \n",
    "\n",
    "Now with our TFIDF matrix and target data, we can split the data into testing and training sets using `train_test_split`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "002fa916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, y_df, random_state=0)     # replace and fill in underscores     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac8fff",
   "metadata": {},
   "source": [
    "12. Import the necessary modules and create a LogisticRegression object. Fit the model according to the X and y training data produced above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8254cf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)     # replace and fill in underscores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87090ab2",
   "metadata": {},
   "source": [
    "13. Now that the model has been trained, obtain the predictions of the model using the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6a75376",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36f36f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2333ef",
   "metadata": {},
   "source": [
    "14. Now we need to evaluate how well the model did. Here, we use three evaluation metrics to assess the performance of our model. These are the metrics we will be working with:\n",
    "\n",
    "+ **Accuracy**: Accuracy is the proportion of correct predictions made by the model out of all the predictions made. It is calculated as the ratio of the number of correct predictions to the total number of predictions.\n",
    "+ **Precision**: Precision is the proportion of true positives out of all the positive predictions made by the model. It is calculated as the ratio of the number of true positives to the total number of positive predictions.\n",
    "+ **Recall**: Recall is the proportion of true positives out of all the actual positive cases in the dataset. It is calculated as the ratio of the number of true positives to the total number of actual positive cases.\n",
    "\n",
    "Import the following metrics and calculate the scores by comparing the test targets to the predicted targets of the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0d7927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function precision_score in module sklearn.metrics._classification:\n",
      "\n",
      "precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "    Compute the precision.\n",
      "    \n",
      "    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "    true positives and ``fp`` the number of false positives. The precision is\n",
      "    intuitively the ability of the classifier not to label as positive a sample\n",
      "    that is negative.\n",
      "    \n",
      "    The best value is 1 and the worst value is 0.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : array-like, default=None\n",
      "        The set of labels to include when ``average != 'binary'``, and their\n",
      "        order if ``average is None``. Labels present in the data can be\n",
      "        excluded, for example to calculate a multiclass average ignoring a\n",
      "        majority negative class, while labels not present in the data will\n",
      "        result in 0 components in a macro average. For multilabel targets,\n",
      "        labels are column indices. By default, all labels in ``y_true`` and\n",
      "        ``y_pred`` are used in sorted order.\n",
      "    \n",
      "        .. versionchanged:: 0.17\n",
      "           Parameter `labels` improved for multiclass problem.\n",
      "    \n",
      "    pos_label : str or int, default=1\n",
      "        The class to report if ``average='binary'`` and the data is binary.\n",
      "        If the data are multiclass or multilabel, this will be ignored;\n",
      "        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "        scores for that label only.\n",
      "    \n",
      "    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "        This parameter is required for multiclass/multilabel targets.\n",
      "        If ``None``, the scores for each class are returned. Otherwise, this\n",
      "        determines the type of averaging performed on the data:\n",
      "    \n",
      "        ``'binary'``:\n",
      "            Only report results for the class specified by ``pos_label``.\n",
      "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "        ``'micro'``:\n",
      "            Calculate metrics globally by counting the total true positives,\n",
      "            false negatives and false positives.\n",
      "        ``'macro'``:\n",
      "            Calculate metrics for each label, and find their unweighted\n",
      "            mean.  This does not take label imbalance into account.\n",
      "        ``'weighted'``:\n",
      "            Calculate metrics for each label, and find their average weighted\n",
      "            by support (the number of true instances for each label). This\n",
      "            alters 'macro' to account for label imbalance; it can result in an\n",
      "            F-score that is not between precision and recall.\n",
      "        ``'samples'``:\n",
      "            Calculate metrics for each instance, and find their average (only\n",
      "            meaningful for multilabel classification where this differs from\n",
      "            :func:`accuracy_score`).\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "        Sets the value to return when there is a zero division. If set to\n",
      "        \"warn\", this acts as 0, but warnings are also raised.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    precision : float (if average is not None) or array of float of shape                 (n_unique_labels,)\n",
      "        Precision of the positive class in binary classification or weighted\n",
      "        average of the precision of each class for the multiclass task.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "        support for each class.\n",
      "    recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the\n",
      "        number of true positives and ``fn`` the number of false negatives.\n",
      "    PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "        an estimator and some data.\n",
      "    PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "        binary class predictions.\n",
      "    multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "        sample.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    When ``true positive + false positive == 0``, precision returns 0 and\n",
      "    raises ``UndefinedMetricWarning``. This behavior can be\n",
      "    modified with ``zero_division``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import precision_score\n",
      "    >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "    >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "    >>> precision_score(y_true, y_pred, average='macro')\n",
      "    0.22...\n",
      "    >>> precision_score(y_true, y_pred, average='micro')\n",
      "    0.33...\n",
      "    >>> precision_score(y_true, y_pred, average='weighted')\n",
      "    0.22...\n",
      "    >>> precision_score(y_true, y_pred, average=None)\n",
      "    array([0.66..., 0.        , 0.        ])\n",
      "    >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "    >>> precision_score(y_true, y_pred, average=None)\n",
      "    array([0.33..., 0.        , 0.        ])\n",
      "    >>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
      "    array([0.33..., 1.        , 1.        ])\n",
      "    >>> # multilabel classification\n",
      "    >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "    >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "    >>> precision_score(y_true, y_pred, average=None)\n",
      "    array([0.5, 1. , 1. ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "help(sklearn.metrics.precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeca77ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function recall_score in module sklearn.metrics._classification:\n",
      "\n",
      "recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "    Compute the recall.\n",
      "    \n",
      "    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "    true positives and ``fn`` the number of false negatives. The recall is\n",
      "    intuitively the ability of the classifier to find all the positive samples.\n",
      "    \n",
      "    The best value is 1 and the worst value is 0.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : array-like, default=None\n",
      "        The set of labels to include when ``average != 'binary'``, and their\n",
      "        order if ``average is None``. Labels present in the data can be\n",
      "        excluded, for example to calculate a multiclass average ignoring a\n",
      "        majority negative class, while labels not present in the data will\n",
      "        result in 0 components in a macro average. For multilabel targets,\n",
      "        labels are column indices. By default, all labels in ``y_true`` and\n",
      "        ``y_pred`` are used in sorted order.\n",
      "    \n",
      "        .. versionchanged:: 0.17\n",
      "           Parameter `labels` improved for multiclass problem.\n",
      "    \n",
      "    pos_label : str or int, default=1\n",
      "        The class to report if ``average='binary'`` and the data is binary.\n",
      "        If the data are multiclass or multilabel, this will be ignored;\n",
      "        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "        scores for that label only.\n",
      "    \n",
      "    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "        This parameter is required for multiclass/multilabel targets.\n",
      "        If ``None``, the scores for each class are returned. Otherwise, this\n",
      "        determines the type of averaging performed on the data:\n",
      "    \n",
      "        ``'binary'``:\n",
      "            Only report results for the class specified by ``pos_label``.\n",
      "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "        ``'micro'``:\n",
      "            Calculate metrics globally by counting the total true positives,\n",
      "            false negatives and false positives.\n",
      "        ``'macro'``:\n",
      "            Calculate metrics for each label, and find their unweighted\n",
      "            mean.  This does not take label imbalance into account.\n",
      "        ``'weighted'``:\n",
      "            Calculate metrics for each label, and find their average weighted\n",
      "            by support (the number of true instances for each label). This\n",
      "            alters 'macro' to account for label imbalance; it can result in an\n",
      "            F-score that is not between precision and recall. Weighted recall\n",
      "            is equal to accuracy.\n",
      "        ``'samples'``:\n",
      "            Calculate metrics for each instance, and find their average (only\n",
      "            meaningful for multilabel classification where this differs from\n",
      "            :func:`accuracy_score`).\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "        Sets the value to return when there is a zero division. If set to\n",
      "        \"warn\", this acts as 0, but warnings are also raised.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    recall : float (if average is not None) or array of float of shape              (n_unique_labels,)\n",
      "        Recall of the positive class in binary classification or weighted\n",
      "        average of the recall of each class for the multiclass task.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "        support for each class.\n",
      "    precision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the\n",
      "        number of true positives and ``fp`` the number of false positives.\n",
      "    balanced_accuracy_score : Compute balanced accuracy to deal with imbalanced\n",
      "        datasets.\n",
      "    multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "        sample.\n",
      "    PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "        an estimator and some data.\n",
      "    PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "        binary class predictions.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "    ``UndefinedMetricWarning``. This behavior can be modified with\n",
      "    ``zero_division``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import recall_score\n",
      "    >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "    >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "    >>> recall_score(y_true, y_pred, average='macro')\n",
      "    0.33...\n",
      "    >>> recall_score(y_true, y_pred, average='micro')\n",
      "    0.33...\n",
      "    >>> recall_score(y_true, y_pred, average='weighted')\n",
      "    0.33...\n",
      "    >>> recall_score(y_true, y_pred, average=None)\n",
      "    array([1., 0., 0.])\n",
      "    >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "    >>> recall_score(y_true, y_pred, average=None)\n",
      "    array([0.5, 0. , 0. ])\n",
      "    >>> recall_score(y_true, y_pred, average=None, zero_division=1)\n",
      "    array([0.5, 1. , 1. ])\n",
      "    >>> # multilabel classification\n",
      "    >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "    >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "    >>> recall_score(y_true, y_pred, average=None)\n",
      "    array([1. , 1. , 0.5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.metrics.recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "334f02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ce86b",
   "metadata": {},
   "source": [
    "In machine learning, multiple metrics for evaluation is typically used as a single metric may not provide a complete picture of the model's performance. Different metrics capture different aspects of model performance, and evaluating a model using multiple metrics helps to provide a better understanding of how well the model is performing.\n",
    "\n",
    "For example, a model with high accuracy may have poor performance for a specific class, or may be overfitting the training data. In this case, we may need to look at other metrics, such as precision and recall, to better understand the model's performance. Similarly, a model with high precision may have low recall, indicating that it is good at identifying positive cases, but is missing some of the actual positive cases.\n",
    "\n",
    "By using multiple metrics for evaluation, we can identify the strengths and weaknesses of the model to make informed decisions about how to improve its performance. It is important to choose evaluation metrics that are relevant to the data you are dealing with, and consider the trade-offs between different metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25aaca2",
   "metadata": {},
   "source": [
    "15. Print out each of the scores for your model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a782fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is  0.9651923076923077\n",
      "The precision score is  0.9661982529434106\n",
      "The recall score is  0.9650986342943855\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy score is \", accuracy)\n",
    "print(\"The precision score is \", precision)\n",
    "print(\"The recall score is \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e38df",
   "metadata": {},
   "source": [
    "Congratulations! You have made it to the end of the quest! Now you have learnt to use your various NLP skills in machine learning projects.\n",
    "\n",
    "**Head back to the StackUp platform** to view the instructions for your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
